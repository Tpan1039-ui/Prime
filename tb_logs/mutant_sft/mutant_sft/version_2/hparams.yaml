adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-07
args: !!python/object:argparse.Namespace
  accelerator: gpu
  accumulate_grad_batches: 8
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-07
  devices: 1
  eval_batch_size: 2
  fasta_file: example_data/example.fasta
  gradient_clip_algorithm: norm
  gradient_clip_value: 1.0
  log_gradient_norm: false
  log_steps: 10
  logger: tensorboard
  logger_project: mutant_sft
  logger_run_name: mutant_sft
  lr: 0.0001
  max_epochs: 200
  max_steps: 1000000
  model_path: AI4Protein/Prime_690M
  monitor: val/pearson
  nodes: 1
  num_workers: 16
  precision: '32'
  save_model_dir: finetune_checkpoint
  save_model_name: mutant_sft
  scheduler_type: constant
  seed: 42
  strategy: auto
  test_file: example_data/test.csv
  tokenizer_path: AI4Protein/Prime_690M
  train_batch_size: 2
  train_file: example_data/train.csv
  trainer_ckpt: null
  valid_file: example_data/valid.csv
  wandb_entity: null
  warmup_max_steps: 5000000
  warmup_steps: 0
  weight_decay: 0.001
eval_determinstic: false
lr: 0.0001
model_path: AI4Protein/Prime_690M
scheduler_type: constant
tokenizer: !!python/object:transformers_modules.AI4Protein.Prime_690M.b76e90d97879f20007deb2ac51692567746640fc.tokenization_proprime.ProPrimeTokenizer
  _added_tokens_decoder:
    0: &id001 !!python/object:tokenizers.AddedToken
      content: <cls>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    1: &id004 !!python/object:tokenizers.AddedToken
      content: <pad>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    2: &id002 !!python/object:tokenizers.AddedToken
      content: <eos>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    3: &id005 !!python/object:tokenizers.AddedToken
      content: <unk>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    32: &id003 !!python/object:tokenizers.AddedToken
      content: <mask>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
  _added_tokens_encoder:
    <cls>: 0
    <eos>: 2
    <mask>: 32
    <pad>: 1
    <unk>: 3
  _additional_special_tokens: []
  _bos_token: null
  _cls_token: *id001
  _decode_use_source_tokenizer: false
  _eos_token: *id002
  _id_to_token:
    0: <cls>
    1: <pad>
    2: <eos>
    3: <unk>
    4: L
    5: A
    6: G
    7: V
    8: S
    9: E
    10: R
    11: T
    12: I
    13: D
    14: P
    15: K
    16: Q
    17: N
    18: F
    19: Y
    20: M
    21: H
    22: W
    23: C
    24: X
    25: B
    26: U
    27: Z
    28: O
    29: .
    30: '-'
    31: <null_1>
    32: <mask>
  _in_target_context_manager: false
  _mask_token: *id003
  _pad_token: *id004
  _pad_token_type_id: 0
  _processor_class: null
  _sep_token: null
  _token_to_id:
    '-': 30
    .: 29
    <cls>: 0
    <eos>: 2
    <mask>: 32
    <null_1>: 31
    <pad>: 1
    <unk>: 3
    A: 5
    B: 25
    C: 23
    D: 13
    E: 9
    F: 18
    G: 6
    H: 21
    I: 12
    K: 15
    L: 4
    M: 20
    N: 17
    O: 28
    P: 14
    Q: 16
    R: 10
    S: 8
    T: 11
    U: 26
    V: 7
    W: 22
    X: 24
    Y: 19
    Z: 27
  _unk_token: *id005
  all_tokens: &id006
  - <cls>
  - <pad>
  - <eos>
  - <unk>
  - L
  - A
  - G
  - V
  - S
  - E
  - R
  - T
  - I
  - D
  - P
  - K
  - Q
  - N
  - F
  - Y
  - M
  - H
  - W
  - C
  - X
  - B
  - U
  - Z
  - O
  - .
  - '-'
  - <null_1>
  - <mask>
  chat_template: null
  clean_up_tokenization_spaces: true
  deprecation_warnings: {}
  init_inputs: !!python/tuple []
  init_kwargs:
    auto_map:
      AutoTokenizer:
      - AI4Protein/Prime_690M--tokenization_proprime.ProPrimeTokenizer
      - null
    clean_up_tokenization_spaces: true
    cls_token: !!python/object:tokenizers.AddedToken
      content: <cls>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    eos_token: !!python/object:tokenizers.AddedToken
      content: <eos>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    mask_token: !!python/object:tokenizers.AddedToken
      content: <mask>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    model_max_length: 1000000000000000019884624838656
    name_or_path: AI4Protein/Prime_690M
    pad_token: !!python/object:tokenizers.AddedToken
      content: <pad>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
    tokenizer_file: null
    unk_token: !!python/object:tokenizers.AddedToken
      content: <unk>
      lstrip: false
      normalized: false
      rstrip: false
      single_word: false
      special: true
  model_input_names:
  - input_ids
  - attention_mask
  model_max_length: 1000000000000000019884624838656
  name_or_path: AI4Protein/Prime_690M
  padding_side: right
  split_special_tokens: false
  tokens_trie: !!python/object:transformers.tokenization_utils.Trie
    _tokens: !!set
      '-': null
      .: null
      <cls>: null
      <eos>: null
      <mask>: null
      <null_1>: null
      <pad>: null
      <unk>: null
      A: null
      B: null
      C: null
      D: null
      E: null
      F: null
      G: null
      H: null
      I: null
      K: null
      L: null
      M: null
      N: null
      O: null
      P: null
      Q: null
      R: null
      S: null
      T: null
      U: null
      V: null
      W: null
      X: null
      Y: null
      Z: null
    data:
      '-':
        ? ''
        : 1
      .:
        ? ''
        : 1
      <:
        c:
          l:
            s:
              '>':
                ? ''
                : 1
        e:
          o:
            s:
              '>':
                ? ''
                : 1
        m:
          a:
            s:
              k:
                '>':
                  ? ''
                  : 1
        n:
          u:
            l:
              l:
                _:
                  '1':
                    '>':
                      ? ''
                      : 1
        p:
          a:
            d:
              '>':
                ? ''
                : 1
        u:
          n:
            k:
              '>':
                ? ''
                : 1
      A:
        ? ''
        : 1
      B:
        ? ''
        : 1
      C:
        ? ''
        : 1
      D:
        ? ''
        : 1
      E:
        ? ''
        : 1
      F:
        ? ''
        : 1
      G:
        ? ''
        : 1
      H:
        ? ''
        : 1
      I:
        ? ''
        : 1
      K:
        ? ''
        : 1
      L:
        ? ''
        : 1
      M:
        ? ''
        : 1
      N:
        ? ''
        : 1
      O:
        ? ''
        : 1
      P:
        ? ''
        : 1
      Q:
        ? ''
        : 1
      R:
        ? ''
        : 1
      S:
        ? ''
        : 1
      T:
        ? ''
        : 1
      U:
        ? ''
        : 1
      V:
        ? ''
        : 1
      W:
        ? ''
        : 1
      X:
        ? ''
        : 1
      Y:
        ? ''
        : 1
      Z:
        ? ''
        : 1
  truncation_side: right
  unique_no_split_tokens: *id006
  verbose: false
warmup_max_steps: 5000000
warmup_steps: 0
weight_dacay: 0.001
